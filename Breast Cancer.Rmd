---
title: "Breast cancer Project"
output: html_notebook
author: "Wagner Rosa"
---

The brca dataset contains information about breast cancer diagnosis biopsy samples for tumors that were determined to be either benign (not cancer) and malignant (cancer). The brca object is a list consisting of:

brca$y: a vector of sample classifications ("B" = benign or "M" = malignant)
brca$x: a matrix of numeric features describing properties of the shape and size of cell nuclei extracted from biopsy microscope images

```{r}
options(digits = 3)
library(matrixStats)
library(tidyverse)
library(caret)
library(dslabs)
data(brca)
```
## Q1: Dimensions and properties
EDA
```{r}
#samples in the dataset
length(brca$y)[1]
#Number of predictors
dim(brca$x)[2]
#Proportion of malignant samples
sum(brca$y == "M")/length(brca$y) 
#Column with highest mean
means_col <- colMeans(brca$x)
which.max(means_col)

#Column with lowest standard deviation
sd_col <- apply(brca$x, 2, sd)
which.min(sd_col)
```
## Q2: Scaling the matrix
Use sweep two times to scale each column: subtract the column mean, then divide by the column standard deviation.

```{r}
x_centered <- sweep(brca$x, 2, colMeans(brca$x))
x_scaled <- sweep(x_centered, 2, colSds(brca$x), FUN = "/")

sd(x_scaled[,1])
median(x_scaled[,1])
```
## Q3: Distance

Calculate the distance between all samples using the scaled matrix.
```{r}
x <- x_scaled
y <- brca$y
d <- dist(x)

as.matrix(d)[1:10,1:10]
# image(as.matrix(d))
malignant <- which(y == "M")
benign <- which(y =="B")
mean(as.matrix(d)[benign,1])
mean(as.matrix(d)[malignant,1])
```
## Q4: Heatmap of features
Heatmap of features

```{r}
library(RColorBrewer)
colors <- brewer.pal(7, "Dark2")[as.numeric(y)]
d_features <- dist(t(x_scaled))
heatmap(as.matrix(d_features), col = brewer.pal(11, "RdBu"), labRow = NA, labCol = NA)
```
## Q5: Hierarchical clustering
Perform hierarchical clustering on the 30 features. Cut the tree into 5 groups.
All but one of the answer options are in the same group.

```{r}
hc <- hclust(d_features)
groups <- cutree(hc, k = 5)
labels <- brca$x[0,1:30]

# install.packages.2 <- function (pkg) if (!require(pkg)) install.packages(pkg);
# install.packages.2('dendextend')
# install.packages.2('colorspace')
library(dendextend)
library(colorspace)

# I'll do this to just 5 clusters for illustrative purposes
k <- 5
cols <- rainbow_hcl(k)
dend <- as.dendrogram(hc)
dend <- color_branches(dend, k = k)
plot(dend)
labels_dend <- labels
groups <- cutree(dend, k = k, order_clusters_as_data = FALSE)
dends <- list()
for(i in 1:k) {
    labels_to_keep <- labels_dend[i != groups]
    dends[[i]] <- prune(dend, labels_to_keep)
}

par(mfrow = c(2,2))
for(i in 1:k) { 
    plot(dends[[i]], 
        main = paste0("Tree number ", i))
}
# p.s.: because we have 3 root o

h <- hclust(d_features)
groups <- cutree(h, k = 5)
split(names(groups), groups)
```

## Q6: PCA: proportion of variance
Perform a principal component analysis of the scaled matrix.
```{r}
#devtools::install_github("kassambara/factoextra")
library(factoextra)
pca <- prcomp(x_scaled, scale = T)
fviz_eig(pca)
get_eig(pca)
summary(pca)
```
## Q7: PCA: plotting PCs
Plot the first two principal components with color representing tumor type (benign/malignant)
```{r}
data.frame(pc_1 = pca$x[,1], pc_2 = pca$x[,2], 
           label = y) %>%
  ggplot(aes(pc_1, pc_2, color = label)) +
  geom_point() 
```
## Q8: PCA: PC boxplot
Make a boxplot of the first 10 PCs grouped by tumor type.
```{r}
data.frame(type = brca$y, pca$x[,1:10]) %>%
    tidyr::gather(key = "PC", value = "value", -type) %>%
    ggplot(aes(PC, value, fill = type)) +
    geom_boxplot()
```
## Q9: Training and test sets
Check that the training and test sets have similar proportions of benign and malignant tumors.

```{r}
library(caret)
set.seed(1, sample.kind = "Rounding")    # if using R 3.6 or later
test_index <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)
test_x <- x_scaled[test_index,]
test_y <- y[test_index]
train_x <- x_scaled[-test_index,]
train_y <- y[-test_index]

summary(train_y)
mean(train_y == "B")

summary(test_y)
mean(test_y == "B")
```

## Q10a: K-means Clustering
The predict_kmeans function defined here takes two arguments - a matrix of observations x and a k-means object k - and assigns each row of x to a cluster from k.
```{r}
predict_kmeans <- function(x, k) {
    centers <- k$centers    # extract cluster centers
    # calculate distance to cluster centers
    distances <- sapply(1:nrow(x), function(i){
                            apply(centers, 1, function(y) dist(rbind(x[i,], y)))
                 })
  max.col(-t(distances))  # select cluster with min distance to center
}

set.seed(3, sample.kind = "Rounding")
k <- kmeans(train_x, centers=2)

# pred <- predict_kmeans(test_x, k)
# 
# mean(pred == as.numeric(test_y))

pred_kmeans <- ifelse(predict_kmeans(test_x, k) == 1, "B", "M")
mean(pred_kmeans == test_y)
```
## Q10b: K-means Clustering
```{r}
#Proportion of benign tumors correctly identified
sensitivity(factor(kmeans_preds), test_y, positive = "B")
sensitivity(factor(kmeans_preds), test_y, positive = "M")
```
## Q11: Logistic regression model
Fit a logistic regression model on the training set using all predictors. Ignore warnings about the algorithm not converging. Make predictions on the test set.
```{r}
train_set <- data.frame(train_x, train_y)
test_set <- data.frame(test_x, test_y )
glm_fit <- glm(train_y ~ ., family = "binomial", data = train_set)
p_hat_glm <- predict(glm_fit, newdata = test_set, type = "response")
pred_glm <- factor(ifelse(p_hat_glm > 0.5, "M", "B"))
confusionMatrix(table(y_hat_glm, test_y))$overall["Accuracy"]

# train_glm <- train(train_x, train_y,
#                      method = "glm")
# glm_preds <- predict(train_glm, test_x)
# mean(glm_preds == test_y)
```
## Q12: LDA and QDA 
Train an LDA model and a QDA model on the training set. Make predictions on the test set using each model.
```{r}
#Alternatively
# models <- c("lda", "qda")
# fits <- lapply(models, function(model){ 
#   print(model)
#   train(train_y ~ ., method = model, data = train_set)
# }) 
# names(fits) <- models
# preds <- sapply(fits, function(f){predict(f, test_set)})
# acc_fits <- sapply(seq(1:2), function(i) {mean(preds[,i] == test_y)})
# knitr::kable(acc_fits)

lda_fit <- train(train_x, train_y,
                     method = "lda")
pred_lda <- predict(lda_fit, test_x)
mean(pred_lda == test_y)

qda_fit <- train(train_x, train_y,
                     method = "qda")
pred_qda <- predict(qda_fit, test_x)
mean(pred_qda == test_y)
```
More things - correlations
```{r}
library(corrplot)
corr_mat <- cor(train_set[,1:ncol(train_x)])
corrplot(corr_mat)
```
## Q13: Loess model
Set the seed to 5, then fit a loess model on the training set with the caret package. You will need to install the gam package if you have not yet done so. Use the default tuning grid. This may take several minutes; ignore warnings. Generate predictions on the test set.
```{r}
set.seed(5, sample.kind = "Rounding") 
loess_fit <- train(train_y ~ ., method = "gamLoess", data = train_set)
pred_loess <- predict(loess_fit, test_set)
mean(pred_loess == test_y)
```
## Q14: K-nearest neighbors model
Set the seed to 7, then train a k-nearest neighbors model on the training set using the caret package. Try odd values of  ð‘˜  from 3 to 21. Use the final model to generate predictions on the test set.
```{r}
set.seed(7, sample.kind = "Rounding")
knn_fit <- train(train_y ~ ., 
                 method = "knn", 
                 tuneGrid = data.frame(k = seq(3, 21, 2)), 
                 data = train_set)
pred_knn <- predict(knn_fit, test_set)
knn_fit
#knn_fit$bestTune
mean(pred_knn == test_y)
```
## Q15a: Random forest model
Set the seed to 9, then train a random forest model on the training set using the caret package. Test mtry values of 3, 5, 7 and 9. Use the argument importance=TRUE so that feature importance can be extracted. Generate predictions on the test set.
```{r}
set.seed(9, sample.kind = "Rounding")
rf_fit <- train(train_y ~ ., 
                method = 'rf', 
                tuneGrid = data.frame(mtry = seq(3, 9, 2)),
                importance = T,
                data = train_set)

rf_fit
pred_rf <- predict(rf_fit, test_set)
mean(pred_rf == test_y)
varImp(rf_fit)
```
## Q15b: Random forest model
Consider the top 10 most important variables in the random forest model.
Which set of features is most important for determining tumor type?
Worst values

## Q16a: Creating an ensemble
```{r}
ensemble <- data.frame(pred_glm, pred_lda, pred_qda, pred_knn, pred_loess, pred_rf, pred_kmeans)
votes <- rowMeans(preds_ensemble == "M")
preds_ensemble <- ifelse(votes > 0.5, "M", "B")
mean(y_hat == test_y)

# ensemble <- cbind(glm = glm_preds == "B", lda = lda_preds == "B", qda = qda_preds == "B", loess = loess_preds == "B", rf = rf_preds == "B", knn = knn_preds == "B", kmeans = kmeans_preds == "B")
# 
# ensemble_preds <- ifelse(rowMeans(ensemble) > 0.5, "B", "M")
# mean(ensemble_preds == test_y)
```
## Q16b: Creating an ensemble
Highest accuracy model
```{r}
models <- c("K means", "Logistic regression", "LDA", "QDA", "Loess", "K nearest neighbors", "Random forest", "Ensemble")
accuracy <- c(mean(pred_kmeans == test_y),
              mean(pred_glm == test_y),
              mean(pred_lda == test_y),
              mean(pred_qda == test_y),
              mean(pred_loess == test_y),
              mean(pred_knn == test_y),
              mean(pred_rf == test_y),
              mean(preds_ensemble == test_y))
data.frame(Model = models, Accuracy = accuracy)
```

